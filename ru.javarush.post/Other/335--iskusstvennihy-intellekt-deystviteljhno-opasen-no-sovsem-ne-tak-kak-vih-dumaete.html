Искусственный интеллект действительно опасен. Но совсем не так, как вы думаете
<p>----------------------------------------</p>
Дебаты на тему искусственного интеллекта и его опасности/пользы для общества начались очень давно. Об ИИ и его влиянии на окружающий мир написано много фантастических книг и научно-популярных статей ...
<p>----------------------------------------</p>
Дебаты на тему искусственного интеллекта и его опасности/пользы для общества начались очень давно. Об ИИ и его влиянии на окружающий мир написано много фантастических книг и научно-популярных статей, снято множество фильмов, время от времени об этом идет речь в блогах и соцсетях. Сегодня мы предлагаем вашему вниманию адаптированный перевод <a href='https://hackernoon.com/the-real-danger-of-artificial-intelligence-its-not-what-you-think-f7fdc7059cf8' target='_blank'>статьи  Жоао Дуарте</a> (João Duarte), в которой он делится своим мнением на эту тему.

<div class='row justify-content-center jr-image-wrap'><div class='col-12 col-sm-10 col-md-8'><img data-id="b02e1ddb-9c09-4d63-9ba8-5b0da9e43036" data-max-width="597" alt=" Искусственный интеллект действительно опасен. Но совсем не так, как вы думаете - 1" src="https://cdn.javarush.com/images/article/b02e1ddb-9c09-4d63-9ba8-5b0da9e43036/1024.jpeg"></div></div>
В начале ноября 2017 года я приехал на «Всемирный симпозиум: Искусственный Интеллект и Инклюзия», в котором приняли участие более 200 человек со всего мира. Лично меня интересовало два вопроса: (1) как создавать ИИ-решения, которые могли бы учитывать общественные культурные ценности и (2) как использовать ИИ в образовании.

В статье я хочу поделиться 5 мыслями, которые я вынес для себя по итогу этого мероприятия.

<h3>1. ИИ может нанести вред (даже если его предназначение заключается совсем в другом)</h3>
Развитие ИИ приносит реальную пользу в таких областях как образование, медицина, медиа, управление и так далее. Но в тоже время, его использование может нести вред и дискриминацию.

<strong>Девиантное поведение ИИ может быть вызвано рядом причин:</strong>

<ul>
<li>использование предвзятых, необъективных или некачественных данных во время обучения;</li>
<li>неправильно сформулированные правила;</li>
<li>использование вне контекста;</li>
<li>создание циклов обратной связи.</li>
</ul>
<table>
<tr>
<td>
<em><strong>Девиантное поведение</strong> (также социальная девиация, отклоняющееся поведение) — это устойчивое поведение личности, отклоняющееся от общепринятых, наиболее распространённых и устоявшихся общественных норм</em>
</td>
</tr>
</table>
<em>Solid Bomb Gold</em> была компанией по производству футболок, которая использовала ИИ при генерировании фраз для печати на своей продукции. Одна из фраз, которую создал алгоритм, звучала так «<strong>Keep Calm and Rape a Lot</strong>» (Сохраняйте спокойствие и насилуйте побольше). И вот такая, казалось бы, незначительная ошибка ИИ стоила компании всего. Этот случай был широко освещен в СМИ и получил соответствующие отклики. После такого скандала площадка Amazon отказала Solid Bomb Gold в сотрудничестве, и компания прекратила свое существование.

<div class='row justify-content-center jr-image-wrap'><div class='col-12 col-sm-10 col-md-8'><img data-id="9b73ba73-3d5f-43b9-bde9-8a1966d5fadd" data-max-width="650" alt=" Искусственный интеллект действительно опасен. Но совсем не так, как вы думаете - 2" src="https://cdn.javarush.com/images/article/9b73ba73-3d5f-43b9-bde9-8a1966d5fadd/1024.jpeg"></div></div>
На самом деле, алгоритм, конечно, не подразумевал создание столь оскорбительного контента. Он действовал просто: выбирал любой глагол из списка, и однажды попал именно на слово «насиловать». Это — отличный пример некачественных данных и плохо определенных правил.

<em>Nikon</em> добавил в свои фотоаппараты функцию, которая предупреждала фотографа, о том, что кто-то моргнул. Но через некоторое время появились отзывы пользователей о неправильном реагировании функции при съемке людей с монголоидным типом внешности. И хотя Nikon — японская компания, похоже, в своих алгоритмах она не учла особенности строения глаз жителей азиатского региона. 

<div class='row justify-content-center jr-image-wrap'><div class='col-12 col-sm-10 col-md-8'><img data-id="3552bfb6-4059-479e-8ece-186b635e16cd" data-max-width="650" alt=" Искусственный интеллект действительно опасен. Но совсем не так, как вы думаете - 3" src="https://cdn.javarush.com/images/article/3552bfb6-4059-479e-8ece-186b635e16cd/1024.jpeg"></div></div>
Помните фильм <em>«Особое мнение»</em> с Томом Крузом? В картине машины предсказывали, где и когда должно произойти преступление, и полиция могла его предупредить. Нечто подобное происходит сейчас. 

<div class='row justify-content-center jr-image-wrap'><div class='col-12 col-sm-10 col-md-8'><img data-id="ef5b2426-fab2-464c-ade8-9504d714b67d" data-max-width="710" alt=" Искусственный интеллект действительно опасен. Но совсем не так, как вы думаете - 4" src="https://cdn.javarush.com/images/article/ef5b2426-fab2-464c-ade8-9504d714b67d/1024.jpeg"></div></div>
Компания <em>PredPol</em> использует ИИ для предвидения преступлений с целью помочь полиции оптимизировать патрульные ресурсы. Данные, которые использовала компания для обучения модели, были взяты из отчетов по преступлениям, зафиксированным полицией. Но далеко не все нарушения могут быть отслежены и запротоколированы. Преступления регистрировались чаще всего в тех районах, где больше всего патрулей. Этот факт может зациклить программу, которая направляет патрульную службу в уже патрулируемые районы. Здесь снова имело место использование необъективных данных, а также петля обратной связи.

Итак, даже если предназначение ИИ заключается совсем в другом, существует вероятность, что он может нанести вред.

<h3>2. ИИ может научиться быть справедливым (порой куда лучше, чем люди)</h3>
Не так уж легко определить, что справедливо, а что — нет. Этот вопрос нужно рассматривать индивидуально, учитывая закон, культурные аспекты, оценку ценностей и различные (обоснованные) точки зрения.

Однако если из всего множества случаев выделить однозначное «добро» или «зло», искусственный интеллект может обучаться на таких примерах. ИИ работает, изучая, как использовать переменные для получения целевого результата (объективных функций). И мы можем улучшить справедливость ИИ, используя такие оценочные данные в качестве объективных функций, или как ограничения переменных.

Если мы намеренно решаем обучить ИИ концепции справедливости, то нужно учитывать, что система будет использовать ее буквальнее, чем люди. Потому что ИИ куда эффективнее, чем мы, следует правилам.

Это демонстрируют многочисленные пример. Так, бразильская компания <em>Desabafo Social</em> провела кампанию, демонстрирующую примеры дискриминации в системах поиска изображений <em>Shutterstock</em> и <em>Google</em>. Один из результатов — если вы отправите запрос «малыш» в Shutterstock, то скорее всего на большинстве фотографий будут белые малыши (<em>а если в Google, то первым будет клип Джастина Бибера Baby, прим. ред.</em>).

<div class='row justify-content-center jr-image-wrap'><div class='col-12 col-sm-10 col-md-8'><img data-id="d8f843af-e67d-4607-8dfc-fbdb9d9664b1" data-max-width="650" alt=" Искусственный интеллект действительно опасен. Но совсем не так, как вы думаете - 5" src="https://cdn.javarush.com/images/article/d8f843af-e67d-4607-8dfc-fbdb9d9664b1/1024.jpeg"></div></div>
Поисковая система не отражает реальное положение дел на Земле, но ее можно этому научить. Если поставить в качестве одной из целей «отражать разнообразие населения земли», ИИ может использовать огромный объем данных, чтобы гарантировать такое представление основываясь на расовых, гендерных и культурных аспектах. В таких вопросах машины эффективнее людей.

В рассмотренном примере, результат работы ИИ дискриминационный, поскольку его так обучили. Но мы всегда можем придумать более эффективные алгоритмы, чтобы избежать этого. 

<h3>3. ИИ заставляет нас пересмотреть этические и моральные принципы</h3>
Как мы видели ранее, ИИ пытается оптимизировать переменные для достижения конкретной цели. В процессе размышления об этих целях то и дело возникают этические и моральные вопросы, которые были на слуху какое-то время назад, а теперь вновь обрели актуальность. Но есть и новые вопросы, чье появление спровоцировали возможности ИИ.

На первом занятии курса юстиции в Гарварде философ <em>Майкл Сандель</em> (Michael Sandel) рассматривает моральную дилемму под названием «<em>Проблема вагонетки</em>» (Trolley problem), которая была впервые поднята в 1905 году. Эта задача моделирует типичную моральную дилемму — можно ли навредить одному или нескольким людям, ради спасения жизни других людей.

<div class='row justify-content-center jr-image-wrap'><div class='col-12 col-sm-10 col-md-8'><img data-id="0b2288e7-efbb-42fe-ae06-8eaf17dabd2b" data-max-width="710" alt=" Искусственный интеллект действительно опасен. Но совсем не так, как вы думаете - 6" src="https://cdn.javarush.com/images/article/0b2288e7-efbb-42fe-ae06-8eaf17dabd2b/1024.jpeg"></div></div>
Итак, ситуация: тяжелая неуправляемая вагонетка несется по рельсам. На пути её следования находятся пять человек, привязанные к рельсам. К счастью, вы можете переключить стрелку — и тогда вагонетка поедет по-другому, запасному пути. К несчастью, на запасном пути находится один человек, также привязанный к рельсам. Каковы ваши действия? Если вы ничего не сделаете, то вагонетка убьет пять человек. Если же переключите стрелку, то она убьет только одного.

С подобной проблемой сталкиваются инженеры автономных транспортных средств, которым необходимо решить, следует ли устанавливать приоритеты в отношении жизни людей внутри или вне транспортного средства.

<h3>4. Не существует понятия «нейтральности ИИ»</h3>
Использование ИИ влияет на нашу жизнь и изменяет ее. Например, мы используем <em>Google Maps</em> для поиска кратчайшего пути и нам больше не нужно спрашивать людей на улице. Так мы меньше общаемся с людьми в незнакомом месте или другом городе во время путешествий.

Или же рассмотрим взаимодействие детей с виртуальными помощниками <em>Siri</em> и <em>Alexa</em>. Это палка о двух концах. С одной стороны, дети становятся более любопытными, потому что могут спросить о чем угодно в любое время суток. С другой стороны, родители отмечают, что дети становятся менее вежливыми. 

<div class='row justify-content-center jr-image-wrap'><div class='col-12 col-sm-10 col-md-8'><img data-id="5ad997a0-9be3-4dae-882e-dbf8f7922989" data-max-width="710" alt=" Искусственный интеллект действительно опасен. Но совсем не так, как вы думаете - 7" src="https://cdn.javarush.com/images/article/5ad997a0-9be3-4dae-882e-dbf8f7922989/1024.jpeg"></div><figcaption><em>Иллюстрация Би Джонсон (Bee Johnson) для Washington Post</em></figcaption></div>

Ведь у них отпадает необходимость использовать слова «пожалуйста» и «спасибо». И это меняет то, как дети учатся взаимодействовать с миром

<table>
<tr>
<td>
<p align="right"><em>«Алгоритмы не нейтральны. Если мы изменим алгоритм, мы изменим реальность»</em></p>
<p align="right"> —  Desabafo Social</p>
</td>
</tr>
</table>
Тем не менее, инженеры искусственного интеллекта могут изменить это, запрограммировав помощников так, чтобы при грубом обращении они отвечали что-то вроде: «Эй, нужно использовать волшебное слово». У нас есть возможность создавать намерения в рамках ИИ и таким образом отстаивать определенную точку зрения.

Итак, хотя ИИ не может быть нейтральным, он может быть инструментом формирования желательного поведения в обществе. И вместо того, чтобы стремиться к нейтралитету, мы должны стремиться к целесообразности.

<h2>5. ИИ нуждается в разнообразии для обеспечения успеха</h2>Алгоритмы искусственного интеллекта <em>всегда</em> будут соответствовать точке зрения своих создателей. Напрашивается вывод, что лучший способ обеспечить успех ИИ —  способствовать разнообразию в этой области. ИИ-инженерами должны становиться люди разных национальностей, мужчины и женщины, с различными расовыми и культурными аспектами. В таком случае получится обеспечить требуемое разнообразие.

Чтобы это произошло, необходимо увеличить доступ к образованию в области ИИ и гарантировать доступ к этим знаниям разным группам людей.

<em>Кейтлин Симиню</em> (Kathleen Siminyu), специалист по анализу данных из Кении, организовала в Найроби отделение организации <em>«Женщина в машинном обучении и анализе данных»</em> (Woman in Machine Learning and Data Science, WiMLDS). Этот проект играет жизненно важную роль в увеличении такого разнообразия в среде ИИ-инженеров.

<div class='row justify-content-center jr-image-wrap'><div class='col-12 col-sm-10 col-md-8'><img data-id="c817759c-eb5f-4430-86db-92c6f2d76984" data-max-width="650" alt=" Искусственный интеллект действительно опасен. Но совсем не так, как вы думаете - 8" src="https://cdn.javarush.com/images/article/c817759c-eb5f-4430-86db-92c6f2d76984/1024.jpeg"></div></div>
Надеюсь, что данная статья окажется полезной и сможет вам пригодится. Я считаю, что понимание вопросов этики при создании ИИ — это первый шаг к созданию более справедливого и инклюзивного искусственного интеллекта.

<table>
<tr>
<th><p>Что еще почитать:
</p>
</th>
</tr>
<tr>
<td>
<p><a href='https://javarush.com/groups/posts/309-deep-learning-iskusstvennihy-intellekt-i-mashinnoe-obuchenie-dlja-chaynikov-obhhjasnenie-na-prim' target='_blank'>Deep Learning, искусственный интеллект и машинное обучение для чайников: объяснение на примере</a></p>
<p><a href='https://javarush.com/groups/posts/140-dewey--pervihy-iskusstvennihy-pisateljh' target='_blank'>https://javarush.com/groups/posts/140-dewey--pervihy-iskusstvennihy-pisateljh</a></p>
<p><a href='https://javarush.com/groups/posts/276-ii-nauchilsja-identificirovatjh-suicidaljhnihe-naklonnosti-proskanirovav-mozg' target='_blank'>ИИ научился идентифицировать суицидальные наклонности, просканировав мозг</a></p>
</td>
</tr>
</table>