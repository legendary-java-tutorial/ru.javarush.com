Apache Cassandra: хранение данных в кластере
<p>----------------------------------------</p>
Распределение данных. 
Согласованность данных при записи. 
Согласованность данных при чтении.
<p>----------------------------------------</p>
<h2>5.1 Распределение данных </h2>

<p>Рассмотрим каким образом данные распределяются в зависимости от ключа по узлам кластера (cluster nodes). Кассандра позволяет задавать стратегию распределения данных. Первая такая стратегия распределяет данные в зависимости от md5 значения ключа — случайный разметчик (random partitioner). Вторая учитывает само битовое представление ключа — порядковый разметчик (byte-ordered partitioner).  </p>

<p>Первая стратегия, в большинстве своем, даёт больше преимуществ, так как вам не нужно заботиться о равномерном распределение данных между серверами и подобных проблемах. Вторую стратегию используют в редких случаях, например если необходимы интервальные запросы (range scan). Важно заметить, что выбор этой стратегии производится перед созданием кластера и фактически не может быть изменён без полной перезагрузки данных. </p>

<p>Для распределения данных кассандра использует технику, известную как согласованное хеширование (consistent hashing). Этот подход позволяет распределить данные между узлами и сделать так, что при добавлении и удалении нового узла количество пересылаемых данных было небольшим. Для этого каждому узлу ставится в соответствие метка (token), которая разбивает на части множество всех md5 значений ключей. Так как в большинстве случаев используется RandomPartitioner, рассмотрим его.</p>
  
<p>Как я уже говорил, RandomPartitioner вычисляет 128-битный md5 для каждого ключа. Для определения в каких узлах будут храниться данные, просто перебираются все метки узлов от меньшего к большему, и, когда значение метки становится больше, чем значение md5 ключа, то этот узел вместе с некоторым количеством последующих узлов (в порядке меток) выбирается для сохранения. Общее число выбранных узлов должно быть равным уровню репликации (replication factor). Уровень репликации задаётся для каждого пространства ключей и позволяет регулировать избыточность данных (data redundancy). </p>

<img data-max-width="1024" data-id="f1d64198-6325-4195-8f44-7f0dc43f6898" src="https://cdn.javarush.com/images/article/f1d64198-6325-4195-8f44-7f0dc43f6898/original.png" alt="">
 
<p>Перед тем, как добавить узел в кластер, необходимо задать ему метку. От того, какой процент ключей покрывает промежуток между этой меткой и следующей, зависит сколько данных будет храниться на узле. Весь набор меток для кластера называется кольцом (ring). </p>

<p>Вот иллюстрация, отображающая при помощи встроенной утилиты nodetool кольцо кластера из 6 узлов с равномерно распределенными метками. </p>

<img data-max-width="1024" data-id="51a9b82a-d1e9-40df-b6c3-143968efb3b9" src="https://cdn.javarush.com/images/article/51a9b82a-d1e9-40df-b6c3-143968efb3b9/original.png" alt="">
 
<h2>5.2 Согласованность данных при записи </h2>

<p>Узлы кластера кассандры равноценны, и клиенты могут соединяться с любым из них, как для записи, так и для чтения. Запросы проходят стадию координации, во время которой, выяснив при помощи ключа и разметчика на каких узлах должны располагаться данные, сервер посылает запросы к этим узлам. Будем называть узел, который выполняет координацию — <strong>координатором</strong> (coordinator), а узлы, которые выбраны для сохранения записи с данным ключом — <strong>узлами-реплик</strong> (replica nodes). Физически координатором может быть один из узлов-реплик — это зависит только от ключа, разметчика и меток. </p>

<p>Для каждого запроса, как на чтение, так и на запись, есть возможность задать уровень согласованности данных. </p>

<p><strong>Для записи</strong> этот уровень будет влиять на количество узлов-реплик, с которых будет ожидаться подтверждение удачного окончания операции (данные записались) перед тем, как вернуть пользователю управление. Для записи существуют такие уровни согласованности: </p>

<ul>
<li><strong>ONE</strong> — координатор шлёт запросы всем узлам-реплик, но, дождавшись подтверждения от первого же узла, возвращает управление пользователю; </li>
<li><strong>TWO</strong> — то же самое, но координатор дожидается подтверждения от двух первых узлов, прежде чем вернуть управление; </li>
<li><strong>THREE</strong> — аналогично, но координатор ждет подтверждения от трех первых узлов, прежде чем вернуть управление; </li>
<li><strong>QUORUM</strong> — собирается кворум: координатор дожидается подтверждения записи от более чем половины узлов-реплик, а именно round(N / 2) + 1, где N — уровень репликации; </li>
<li><strong>LOCAL_QUORUM</strong> — координатор дожидается подтверждения от более чем половины узлов-реплик в том же центре обработки данных, где расположен координатор (для каждого запроса потенциально свой). Позволяет избавиться от задержек, связанных с пересылкой данных в другие центры обработки данных. Вопросы работы с многими центрами обработки данных рассматриваются в этой статье вскользь; </li>
<li><strong>EACH_QUORUM</strong> — кооринатор дожидается подтверждения от более чем половины узлов-реплик в каждом центре обработки данных независимо; </li>
<li><strong>ALL</strong> — координатор дожидается подтверждения от всех узлов-реплик; </li>
<li><strong>ANY</strong> — даёт возможность записать данные, даже если все узлы-реплики не отвечают. Координатор дожидается или первого ответа от одного из узлов-реплик, или когда данные сохранятся при помощи направленной отправки (hinted handoff) на координаторе. </li>
</ul>
 
<img data-max-width="1024" data-id="521841e0-1890-4d30-8b36-eadbd4a7b1f4" src="https://cdn.javarush.com/images/article/521841e0-1890-4d30-8b36-eadbd4a7b1f4/original.png" alt=""> 

<h2>5.3 Согласованность данных при чтении </h2>

<p><strong>Для чтения</strong> уровень согласованности будет влиять на количество узлов-реплик, с которых будет производиться чтение. Для чтения существуют такие уровни согласованности: </p>

<ul>
<li><strong>ONE</strong> — координатор шлёт запросы к ближайшему узлу-реплике. Остальные реплики также читаются в целях чтения с исправлением (read repair) с заданной в конфигурации кассандры вероятностью; </li>
<li><strong>TWO</strong> — то же самое, но координатор шлёт запросы к двум ближайшим узлам. Выбирается то значение, которое имеет большую метку времени; </li>
<li><strong>THREE</strong> — аналогично предыдущему варианту, но с тремя узлами; </li>
<li><strong>QUORUM</strong> — собирается кворум, то есть координатор шлёт запросы к более чем половине узлов-реплик, а именно round(N / 2) + 1, где N — уровень репликации; </li>
<li><strong>LOCAL_QUORUM</strong> — собирается кворум в том центре обработки данных, в котором происходит координация, и возвращаются данные с последней меткой времени; </li>
<li><strong>EACH_QUORUM</strong> — координатор возвращает данные после собрания кворума в каждом из центров обработки данных; </li>
<li><strong>ALL</strong> — координатор возвращает данные после прочтения со всех узлов-реплик. </li>
</ul>
 
<img data-max-width="1024" data-id="ed9df03b-6ba0-4e7a-9281-59e4fb476fc8" src="https://cdn.javarush.com/images/article/ed9df03b-6ba0-4e7a-9281-59e4fb476fc8/original.png" alt="">

<p>Таким образом, можно регулировать временные задержки операций чтения, записи и настраивать согласованность (tune consistency), а также доступность (availability) каждой из видов операций. По сути, доступность напрямую зависит от уровня согласованности операций чтения и записи, так как он определяет, сколько узлов-реплик может выйти из строя, и при этом эти операции все ещё будут подтверждены. </p>

<p>Если число узлов, с которых приходит подтверждения о записи, в сумме с числом узлов, с которых происходит чтение, больше, чем уровень репликации, то у нас есть гарантия, что после записи новое значение всегда будет прочитано, и это называется строгой согласованностью (strong consistency). При отсутствии строгой согласованности существует возможность того, что операция чтения возвратит устаревшие данные. </p>

<p>В любом случае, значение в конце концов распространится между репликами, но уже после того, как закончится координационное ожидание. Такое распространение называется итоговой согласованностью (eventual consistency). Если не все узлы-реплики будут доступны во время записи, то рано или поздно будут задействованы средства восстановления, такие как чтение с исправлением и анти-энтропийное восстановление узла (anti-entropy node repair). Об этом чуть позже. </p>

<p>Таким образом, при уровне согласованности QUORUM на чтение и на запись всегда будет поддерживаться строгая согласованность, и это будет некий баланс между задержкой операции чтения и записи. При записи ALL, а чтении ONE будет строгая согласованность, и операции чтения будут выполняться быстрее и будут иметь большую доступность, то есть количество вышедших из строя узлов, при котором чтение все еще будет выполнено, может быть большим, чем при QUORUM.  </p>

<p>Для операций записи же потребуются все рабочие узлы-реплик. При записи ONE, чтении ALL тоже будет строгая согласованность, и операции записи будут выполняться быстрее и доступность записи будет большой, ведь будет достаточно подтвердить лишь, что операция записи прошла хотя бы на одном из серверов, а чтение — медленней и требовать всех узлов-реплик. Если же к приложению нету требования о строгой согласованности, то появляется возможность ускорить и операции чтения и операции записи, а также улучшить доступность за счет выставления меньших уровней согласованности.</p>